// This file was generated by lezer-generator. You probably shouldn't edit it.
import {LRParser} from "@lezer/lr"
import {noteContent} from "./external-tokens.js"
export const parser = LRParser.deserialize({
  version: 14,
  states: "!jQQOPOOOVOPO'#C`O[OQO'#C_OOOO'#Cc'#CcQQOPOOOaOPO,58zOOOO,58y,58yOOOO-E6a-E6aOfOPO1G.fOOOQ7+$Q7+$QOnOPO7+$QOOOQ<<Gl<<Gl",
  stateData: "s~OXPO~OYTO~OPUO~OTWO~OUYOXXO~OXZO~O",
  goto: "gWPPPX]PPaTROSTQOSQSORVS",
  nodeNames: "âš  NoteContent Document Note NoteDelimiter NoteLanguage Auto",
  maxTerm: 10,
  skippedNodes: [0],
  repeatNodeCount: 1,
  tokenData: "/W~RcYZ!^}!O!c#V#W!n#W#X$[#X#Y$}#Z#[&Y#[#]&{#^#_'_#_#`(u#`#a)_#a#b)q#d#e*p#f#g,[#g#h,k#h#i-j#j#k.i#l#m'R#m#n.o%&x%&y.u~!cOX~~!fP#T#U!i~!nOU~~!qR#`#a!z#d#e#o#g#h#u~!}P#c#d#Q~#TP#^#_#W~#ZP#i#j#^~#aP#f#g#d~#gP#X#Y#j~#oOT~~#rP#d#e#j~#xQ#[#]$O#g#h#j~$RP#T#U$U~$XP#f#g#o~$_Q#T#U$e#]#^$q~$hP#f#g$k~$nP#h#i#j~$tP#Y#Z$w~$zP#Y#Z#j~%QQ#`#a%W#f#g%p~%ZP#]#^%^~%aP#l#m%d~%gP#]#^%j~%mP#f#g#j~%sP#`#a%v~%yP#T#U%|~&PP#b#c&S~&VP#Z#[#j~&]Q#c#d%p#f#g&c~&fP#c#d&i~&lP#c#d&o~&rP#j#k&u~&xP#m#n#j~'OP#h#i'R~'UP#a#b'X~'[P#`#a#j~'bQ#T#U'h#g#h(f~'kP#j#k'n~'qP#T#U't~'yPT~#g#h'|~(PP#V#W(S~(VP#f#g(Y~(]P#]#^(`~(cP#d#e$k~(iQ#c#d(o#l#m#j~(rP#b#c#j~(xP#c#d({~)OP#h#i)R~)UP#`#a)X~)[P#]#^(o~)bP#X#Y)e~)hP#n#o)k~)nP#X#Y%j~)tP#T#U)w~)zQ#f#g*Q#h#i*j~*TP#_#`*W~*ZP#W#X*^~*aP#c#d*d~*gP#k#l(o~*mP#[#]#j~*sR#[#]#o#c#d*|#m#n+x~+PP#k#l+S~+VP#X#Y+Y~+]P#f#g+`~+cP#g#h+f~+iP#[#]+l~+oP#X#Y+r~+uP#`#a'X~+{P#h#i,O~,RP#[#],U~,XP#c#d(o~,_P#i#j,b~,eQ#U#V&u#g#h$k~,nS#V#W,z#[#]+l#e#f'X#k#l-^~,}P#T#U-Q~-TP#`#a-W~-ZP#T#U#j~-aP#]#^-d~-gP#Y#Z$k~-mS#X#Y-y#c#d'R#g#h.P#m#n.V~-|P#l#m$k~.SP#l#m#j~.YP#d#e.]~.`P#X#Y.c~.fP#g#h'|~.lP#i#j#d~.rP#T#U'R~.xP%&x%&y.{~/OP%&x%&y/R~/WOY~",
  tokenizers: [0, noteContent],
  topRules: {"Document":[0,2]},
  tokenPrec: 0
})
