// This file was generated by lezer-generator. You probably shouldn't edit it.
import {LRParser} from "@lezer/lr"
import {noteContent} from "./external-tokens.js"
export const parser = LRParser.deserialize({
  version: 14,
  states: "!jQQOPOOOVOPO'#C`O[OQO'#C_OOOO'#Cc'#CcQQOPOOOaOPO,58zOOOO,58y,58yOOOO-E6a-E6aOfOPO1G.fOOOQ7+$Q7+$QOnOPO7+$QOOOQ<<Gl<<Gl",
  stateData: "s~OXPO~OYTO~OPUO~OTWO~OUYOXXO~OXZO~O",
  goto: "gWPPPX]PPaTROSTQOSQSORVS",
  nodeNames: "âš  NoteContent Document Note NoteDelimiter NoteLanguage Auto",
  maxTerm: 10,
  skippedNodes: [0],
  repeatNodeCount: 1,
  tokenData: ".q~RcYZ!^}!O!c#V#W!n#W#X$[#X#Y$}#Z#[%m#[#]&`#^#_&r#_#`(Y#`#a(r#a#b)[#d#e*Z#f#g+u#g#h,U#h#i-T#j#k.S#l#m&f#m#n.Y%&x%&y.`~!cOX~~!fP#T#U!i~!nOU~~!qR#`#a!z#d#e#o#g#h#u~!}P#c#d#Q~#TP#^#_#W~#ZP#i#j#^~#aP#f#g#d~#gP#X#Y#j~#oOT~~#rP#d#e#j~#xQ#[#]$O#g#h#j~$RP#T#U$U~$XP#f#g#o~$_Q#T#U$e#]#^$q~$hP#f#g$k~$nP#h#i#j~$tP#Y#Z$w~$zP#Y#Z#j~%QP#f#g%T~%WP#`#a%Z~%^P#T#U%a~%dP#b#c%g~%jP#Z#[#j~%pQ#c#d%T#f#g%v~%yP#c#d%|~&PP#c#d&S~&VP#j#k&Y~&]P#m#n#j~&cP#h#i&f~&iP#a#b&l~&oP#`#a#j~&uQ#T#U&{#g#h'y~'OP#j#k'R~'UP#T#U'X~'^PT~#g#h'a~'dP#V#W'g~'jP#f#g'm~'pP#]#^'s~'vP#d#e$k~'|Q#c#d(S#l#m#j~(VP#b#c#j~(]P#c#d(`~(cP#h#i(f~(iP#`#a(l~(oP#]#^(S~(uP#X#Y(x~({P#n#o)O~)RP#X#Y)U~)XP#f#g#j~)_P#T#U)b~)eQ#f#g)k#h#i*T~)nP#_#`)q~)tP#W#X)w~)zP#c#d)}~*QP#k#l(S~*WP#[#]#j~*^R#[#]#o#c#d*g#m#n+c~*jP#k#l*m~*pP#X#Y*s~*vP#f#g*y~*|P#g#h+P~+SP#[#]+V~+YP#X#Y+]~+`P#`#a&l~+fP#h#i+i~+lP#[#]+o~+rP#c#d(S~+xP#i#j+{~,OQ#U#V&Y#g#h$k~,XS#V#W,e#[#]+V#e#f&l#k#l,w~,hP#T#U,k~,nP#`#a,q~,tP#T#U#j~,zP#]#^,}~-QP#Y#Z$k~-WS#X#Y-d#c#d&f#g#h-j#m#n-p~-gP#l#m$k~-mP#l#m#j~-sP#d#e-v~-yP#X#Y-|~.PP#g#h'a~.VP#i#j#d~.]P#T#U&f~.cP%&x%&y.f~.iP%&x%&y.l~.qOY~",
  tokenizers: [0, noteContent],
  topRules: {"Document":[0,2]},
  tokenPrec: 0
})
